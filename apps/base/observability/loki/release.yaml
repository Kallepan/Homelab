# ---
# apiVersion: helm.toolkit.fluxcd.io/v2
# kind: HelmRelease
# metadata:
#   name: loki
#   namespace: observability
# spec:
#   interval: 1h
#   chart:
#     spec:
#       chart: loki
#       version: "6.x"
#       sourceRef:
#         kind: HelmRepository
#         name: grafana
#         namespace: observability
#   install:
#     crds: CreateReplace
#   upgrade:
#     crds: CreateReplace
#   values:
#     chunksCache:
#       allocatedMemory: 2048

#     loki:
#       auth_enabled: false
#       schemaConfig:
#         configs:
#           - from: "2024-04-01"
#             store: tsdb
#             object_store: s3
#             schema: v13
#             index:
#               prefix: loki_index_
#               period: 24h
#       rulerConfig:
#         alertmanager_url: http://prometheus-kube-prometheus-alertmanager.observability.svc.cluster.local:9093
#         enable_alertmanager_v2: true
#         enable_api: true
#         rule_path: /var/loki/rules-temp
#         ring:
#           kvstore:
#             store: inmemory
#         storage:
#           type: local
#           local:
#             directory: /etc/loki/rules
#       ingester:
#         chunk_encoding: snappy
#       querier:
#         # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
#         max_concurrent: 3
#       pattern_ingester:
#         enabled: true
#       limits_config:
#         allow_structured_metadata: true
#         volume_enabled: true
#         retention_period: 168h # 7 days
#       storage_config:
#         aws:
#           s3: https://loki:tkEmZByzhmFvYL1TisUKu1B5lPwgrS0X@s3.infra.server.home:9000/loki-chunks
#           s3forcepathstyle: true
#       storage:
#         type: s3
#         bucketNames:
#           chunks: loki-chunks
#           ruler: loki-ruler
#           admin: loki-admin
#         s3:
#           # s3 URL can be used to specify the endpoint, access key, secret key, and bucket name this works well for S3 compatible storage or if you are hosting Loki on-premises and want to use S3 as the storage backend. Either use the s3 URL or the individual fields below (AWS endpoint, region, secret).
#           s3: https://loki:tkEmZByzhmFvYL1TisUKu1B5lPwgrS0X@s3.infra.server.home:9000/loki-chunks
#           # Not sure, if this is needed
#           access_key_id: loki
#           secret_access_key: tkEmZByzhmFvYL1TisUKu1B5lPwgrS0X
#           endpoint: https://s3.infra.server.home:9000
#           signatureVersion: "v4"
#           region: null
#           # Is there another way to specify the access key and secret key?
#           s3ForcePathStyle: true
#           insecure: false
#           http_config:
#             idle_conn_timeout: 90s
#             response_header_timeout: 0s
#             insecure_skip_verify: true

#     deploymentMode: Distributed

#     minio:
#       enabled: false

#     ruler:
#       enabled: true
#       replicas: 2
#       maxUnavailable: 1
#       directories:
#         fake:
#           # Enable this to test the rule files
#           # test.yaml: |
#           #   groups:
#           #     - name: should_fire
#           #       rules:
#           #         - alert: HighPercentageError
#           #           expr: |
#           #             sum(rate({job=~".+"} |= "error" [5m])) by (job)
#           #               /
#           #             sum(rate({job=~".+"}[5m])) by (job)
#           #               > 0.05
#           #           for: 10m
#           #           labels:
#           #               severity: page
#           #           annotations:
#           #               summary: High request latency
#           high_warn_rate.yaml: |
#             groups:
#               - name: high_warn_rate
#                 rules:
#                   - alert: HighPercentageWarn
#                     expr: |
#                       sum(rate({job=~".+"} |= "warn" or "warning" [5m])) by (namespace, pod)
#                         /
#                       sum(rate({job=~".+"}[5m])) by (namespace, pod)
#                         > 0.10
#                     for: 10m
#                     labels:
#                       severity: warning
#                       namespace: "{{ $labels.namespace }}"
#                       pod: "{{ $labels.pod }}"
#                       cluster: "{{ $labels.cluster }}"
#                       instance: "{{ $labels.instance }}"
#                       source: "loki"
#                     annotations:
#                       summary: High percentage of warn logs detected in job {{ $labels.job }}.
#                       description: "{{ $labels.pod }} in {{ $labels.namespace }} has a warning rate above 10% over the last 10 minutes."
#           high_error_rate.yaml: |
#             groups:
#               - name: high_error_rate
#                 rules:
#                   - alert: HighPercentageError
#                     expr: |
#                       sum(rate({job=~".+", pod !~ "\\S+-postgres-\\d"} |= "error" [5m])) by (namespace, pod)
#                         /
#                       sum(rate({job=~".+"}[5m])) by (namespace, pod)
#                         > 0.10
#                     for: 10m
#                     labels:
#                       severity: critical
#                       namespace: "{{ $labels.namespace }}"
#                       pod: "{{ $labels.pod }}"
#                       cluster: "{{ $labels.cluster }}"
#                       instance: "{{ $labels.instance }}"
#                       source: "loki"
#                     annotations:
#                       summary: High percentage of error logs detected in job {{ $labels.job }}.
#                       description: "{{ $labels.pod }} in {{ $labels.namespace }} has an error rate above 10% over the last 10 minutes."
#               - name: high_error_rate_postgres
#                 rules:
#                   - alert: HighPercentageError
#                     expr: |
#                       sum(rate({job=~".+", pod =~ "\\S+-postgres-\\d"} |~ "\"level\":\"error\"" [5m])) by (namespace, pod)
#                         /
#                       sum(rate({job=~".+"}[5m])) by (namespace, pod)
#                         > 0.10
#                     for: 10m
#                     labels:
#                       severity: critical
#                       namespace: "{{ $labels.namespace }}"
#                       pod: "{{ $labels.pod }}"
#                       cluster: "{{ $labels.cluster }}"
#                       instance: "{{ $labels.instance }}"
#                       source: "loki"
#                     annotations:
#                       summary: High percentage of error logs detected in job {{ $labels.job }}.
#                       description: "{{ $labels.pod }} in {{ $labels.namespace }} has an error rate above 10% over the last 10 minutes."
#           unauthorized_access_attempts.yaml: |
#             groups:
#               - name: UnauthorizedAccessAttempts
#                 rules:
#                   - alert: UnauthorizedAccessAttempts
#                     expr: |
#                       sum by(job) (rate({job=~".+"} |= `unauthorized` or `failed login` or `invalid credentials` [1m])) > 2
#                     for: 5m
#                     labels:
#                       severity: critical
#                       namespace: "{{ $labels.namespace }}"
#                       pod: "{{ $labels.pod }}"
#                       cluster: "{{ $labels.cluster }}"
#                       instance: "{{ $labels.instance }}"
#                       source: "loki"
#                     annotations:
#                       summary: Unauthorized access attempts detected in job {{ $labels.job }}
#                       description: More than 2 unauthorized access attempts in the last 5 minutes.
#           high_log_volume.yaml: |
#             groups:
#               - name: HighLogVolume
#                 rules:
#                   - alert: LogVolumeAnomaly
#                     expr: |
#                       sum(rate({job=~".+"}[5m])) by (namespace,pod)
#                         >
#                       (sum(rate({job=~".+"}[5m])) by (namespace,pod) * 1.5)
#                     for: 10m
#                     labels:
#                       severity: warning
#                       namespace: "{{ $labels.namespace }}"
#                       pod: "{{ $labels.pod }}"
#                       cluster: "{{ $labels.cluster }}"
#                       instance: "{{ $labels.instance }}"
#                       source: "loki"
#                     annotations:
#                       summary: Log volume anomaly detected for job {{ $labels.job }}
#                       description: Unusual increase in log volume detected for job {{ $labels.job }} over the last 10 minutes.
#     ingester:
#       replicas: 3 # To ensure data durability with replication
#     querier:
#       replicas: 3 # Improve query performance via parallelism
#       maxUnavailable: 2
#     queryFrontend:
#       replicas: 2
#       maxUnavailable: 1
#     queryScheduler:
#       replicas: 2
#     distributor:
#       replicas: 3
#       maxUnavailable: 2
#     compactor:
#       replicas: 1
#     indexGateway:
#       replicas: 2
#       maxUnavailable: 1

#     bloomPlanner:
#       replicas: 0
#     bloomBuilder:
#       replicas: 0
#     bloomGateway:
#       replicas: 0

#     backend:
#       replicas: 0
#     read:
#       replicas: 0
#     write:
#       replicas: 0

#     singleBinary:
#       replicas: 0
